{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://www.bilibili.com/video/BV1nV411a74n?t=70.1"
      ],
      "metadata": {
        "id": "yXvCiDnyIzBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 注意力机制\n",
        "\n",
        "- **注意力分数，求加权和**\n",
        "- **输入和输出的维度相同，很容易把模型做深**\n",
        "\n",
        "找到哪更应该被注意 - 找注意力\n",
        "\n",
        "然后将原来的内容，咱找注意力加权求和，得到真正要被注意的内容\n",
        "\n",
        "比如找到消极或积极的态度\n",
        "\n",
        "- 今天天气不错\n",
        "- 今天天气太糟了\n",
        "- 今天天气\n",
        "- 不错\n",
        "\n",
        "最早用在机器翻译，用在seq2seq模型，从不定长到不定长的序列，分为两部分encoder（对上文的理解）和decoder\n",
        "\n",
        "encoder是一次性读完，decoder是一个字一个字输出\n",
        "\n",
        "- t0: decoder(对上文的理解，起始字符) -> I\n",
        "- t1: decoder(对上文的理解（产生变化），I) -> love\n",
        "- t2: decoder(对上文的理解（产生变化），love) -> you\n",
        "- t3: decoder(对上文的理解（产生变化），you) -> 中止字符\n",
        "\n",
        "找到注意力（[我的理解，爱的理解，你的理解]: key，起始字符: Query）-> 注意力向量 = [0,98, 0.01, 0.01]\n",
        "\n",
        "注意力 * 每个字的理解 = sum([0.98, 0.01, 0.01] * [我的理解，爱的理解，你的理解]: Value) = sum( 0.98 * 我 + 0.01 * 爱 + 0.01 * 你) = 根据起始字符进行注意力后的理解\n",
        "\n",
        "根据起始字符进行注意力后的理解 = 注意力理解（[我的理解，爱的理解，你的理解]，起始字符）\n",
        "\n",
        "- t0: decoder(根据起始字符进行注意力后的理解) -> I\n",
        "- t1: decoder(根据I进行注意力后的理解，I) -> love\n",
        "- t2: decoder(根据love进行注意力后的理解，love) -> you\n",
        "- t3: decoder(根据you进行注意力后的理解，you) -> 中止字符\n",
        "\n",
        "### Self attention\n",
        "\n",
        "没有decode的过程\n",
        "\n",
        "|       |  我的理解|爱的理解|你的理解|\n",
        "|  --   |  --     | --    |   --  |\n",
        "|我的理解|    1.0  |    0.2 |     0.1|\n",
        "|爱的理解 |   0.2 |     1.0  |    0.2|\n",
        "|你的理解 |   0.1 |     0.0  |    1.0|\n",
        "\n",
        "输入：我的理解，爱的理解，你的理解\n",
        "输出：我的新理解，爱的新理解，你的新理解。输出融入了输入中其他向量的信息，更能体会到上下文语境"
      ],
      "metadata": {
        "id": "X_kD_LXiJXrN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jTP9nGs3IkH5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "sequence_length = 10\n",
        "vector_size = 32\n",
        "\n",
        "x = tf.random.uniform((batch_size, sequence_length, vector_size))"
      ],
      "metadata": {
        "id": "Z4QVzqCLP-u_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZOB266NgUvf",
        "outputId": "c0e75fa0-0b17-4dd9-aa57-5dc0a1bbcfa5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 线性映射，把x转换成q,k,v\n",
        "key = tf.keras.layers.Dense(vector_size)(x)\n",
        "value = tf.keras.layers.Dense(vector_size)(x)\n",
        "query = tf.keras.layers.Dense(vector_size)(x)\n",
        "key.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOa1IOckgW5N",
        "outputId": "2d92079d-9d6f-4df9-c083-e626211c2924"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{n}})V$$"
      ],
      "metadata": {
        "id": "3xkpROQ2IymC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mat_mul = tf.matmul(query, key, transpose_b=True)\n",
        "mat_mul.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOSKXxEagiRz",
        "outputId": "0c810c30-cf79-4897-e91a-938fe852668d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "atten的输出为什么是10*10：因为针对的是每个句子中10个单词进行attention，所以输出的就是这10个单词两两cross cos similar的分数"
      ],
      "metadata": {
        "id": "d3PZAPayhOps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = vector_size\n",
        "atten = mat_mul / tf.sqrt(tf.cast(n, tf.float32))\n",
        "atten = tf.nn.softmax(atten)\n",
        "# 乘上v\n",
        "y = tf.matmul(atten, value)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjw05dcPhDil",
        "outputId": "9687c850-d463-4eba-86b4-449a0c890460"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "封装成函数\n"
      ],
      "metadata": {
        "id": "Frzw6S6GzNDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention(x):\n",
        "  key = tf.keras.layers.Dense(vector_size)(x)\n",
        "  value = tf.keras.layers.Dense(vector_size)(x)\n",
        "  query = tf.keras.layers.Dense(vector_size)(x)\n",
        "  mat_mul = tf.matmul(query, key, transpose_b=True)\n",
        "  n = vector_size\n",
        "  atten = mat_mul / tf.sqrt(tf.cast(n, tf.float32))\n",
        "  atten = tf.nn.softmax(atten)\n",
        "  # 乘上v\n",
        "  y = tf.matmul(atten, value)\n",
        "  return y\n",
        "\n",
        "self_attention(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbSLVtJWjhKT",
        "outputId": "040b5a0e-5bab-4e5a-d2fa-f6a4fc57a73d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head\n",
        "\n",
        "heads数量需要被vactor_size整除"
      ],
      "metadata": {
        "id": "NzRKM9WMzo8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "sequence_length = 10\n",
        "vector_size = 32\n",
        "heads = 4\n",
        "\n",
        "x = tf.random.uniform((batch_size, sequence_length, vector_size))\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvi1rtPRziti",
        "outputId": "5138f560-0d18-4244-f3ca-b78949642b61"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-head输入\n",
        "x_mh = tf.reshape(x, [batch_size, sequence_length, heads, vector_size // heads])\n",
        "x_mh.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz8X6GUA1hR8",
        "outputId": "e9114e32-c997-4384-8d2c-b46916ac4c5d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "每个头可以单独的算一个新句子, 现在有4 * 4 = 16个句子\n",
        "\n",
        "把新句子移到第二个维度"
      ],
      "metadata": {
        "id": "97PEf62f2I07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_mh = tf.transpose(x_mh, (0, 2, 1, 3))\n",
        "x_mh.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CvlMnNn14BX",
        "outputId": "48ba01f3-ac58-4339-9d0f-fcac236bdde6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 4, 10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = tf.keras.layers.Dense(vector_size // heads)(x_mh)\n",
        "value = tf.keras.layers.Dense(vector_size // heads)(x_mh)\n",
        "query = tf.keras.layers.Dense(vector_size // heads)(x_mh)\n",
        "key.shape, value.shape, query.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CkxvdPg21DG",
        "outputId": "228591bc-7996-468d-e13b-5480a0062e4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([4, 4, 10, 8]),\n",
              " TensorShape([4, 4, 10, 8]),\n",
              " TensorShape([4, 4, 10, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat_mul = tf.matmul(query, key, transpose_b=True)\n",
        "n = vector_size\n",
        "atten = mat_mul / tf.sqrt(tf.cast(n, tf.float32))\n",
        "atten = tf.nn.softmax(atten)\n",
        "# 乘上v\n",
        "y = tf.matmul(atten, value)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hdN6fOF3azp",
        "outputId": "70e9fe72-5221-43fd-d1b8-0c78fc5d19bb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 4, 10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "需要转回到原来的输入shape"
      ],
      "metadata": {
        "id": "HcrADihA3nca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.transpose(y, (0, 2, 1, 3))\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd1vqnXv3min",
        "outputId": "ec141ed7-b3c7-4718-d900-a86dd5fcfd81"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "再把后面的两个维度合并在一起"
      ],
      "metadata": {
        "id": "ZIiQPqDL3-gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.reshape(y, (batch_size, sequence_length, vector_size))"
      ],
      "metadata": {
        "id": "VouY971937aa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "封装成函数"
      ],
      "metadata": {
        "id": "htWQache4iYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_head_self_attention(x):\n",
        "  x_mh = tf.reshape(x, [batch_size, sequence_length, heads, vector_size // heads])\n",
        "  x_mh = tf.transpose(x_mh, (0, 2, 1, 3))\n",
        "  key = tf.keras.layers.Dense(vector_size // heads)(x_mh)\n",
        "  value = tf.keras.layers.Dense(vector_size // heads)(x_mh)\n",
        "  query = tf.keras.layers.Dense(vector_size // heads)(x_mh)\n",
        "  mat_mul = tf.matmul(query, key, transpose_b=True)\n",
        "  n = vector_size\n",
        "  atten = mat_mul / tf.sqrt(tf.cast(n, tf.float32))\n",
        "  atten = tf.nn.softmax(atten)\n",
        "  # 乘上v\n",
        "  y = tf.matmul(atten, value)\n",
        "  y = tf.transpose(y, (0, 2, 1, 3))\n",
        "  y = tf.reshape(y, (batch_size, sequence_length, vector_size))\n",
        "  return y\n",
        "\n",
        "multi_head_self_attention(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tWncqV94hLw",
        "outputId": "73afe92a-c34c-42c6-a97a-b13b8ea5ca50"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "封装成tf的model"
      ],
      "metadata": {
        "id": "cA5xpYsp5KKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(tf.keras.Model):\n",
        "  def __init__(self, vector_size, heads=1):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.vector_size = vector_size\n",
        "    self.heads = heads\n",
        "    self.key = tf.keras.layers.Dense(vector_size // heads)\n",
        "    self.value = tf.keras.layers.Dense(vector_size // heads)\n",
        "    self.query = tf.keras.layers.Dense(vector_size // heads)\n",
        "\n",
        "  def call(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    sequence_length = x.shape[1]\n",
        "    x_mh = tf.reshape(x, [batch_size, sequence_length, self.heads, self.vector_size // self.heads])\n",
        "    x_mh = tf.transpose(x_mh, (0, 2, 1, 3))\n",
        "    key = tf.keras.layers.Dense(self.vector_size // self.heads)(x_mh)\n",
        "    value = tf.keras.layers.Dense(self.vector_size // self.heads)(x_mh)\n",
        "    query = tf.keras.layers.Dense(self.vector_size // self.heads)(x_mh)\n",
        "    mat_mul = tf.matmul(query, key, transpose_b=True)\n",
        "    n = self.vector_size\n",
        "    atten = mat_mul / tf.sqrt(tf.cast(n, tf.float32))\n",
        "    atten = tf.nn.softmax(atten)\n",
        "    # 乘上v\n",
        "    y = tf.matmul(atten, value)\n",
        "    y = tf.transpose(y, (0, 2, 1, 3))\n",
        "    y = tf.reshape(y, (batch_size, sequence_length, self.vector_size))\n",
        "    return y"
      ],
      "metadata": {
        "id": "mkPoS9Eq5E_g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "sequence_length = 10\n",
        "vector_size = 32\n",
        "heads = 4\n",
        "\n",
        "x = tf.random.uniform((batch_size, sequence_length, vector_size))\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cz7ABQ960Nu",
        "outputId": "dd699a77-174b-431e-b14b-9ab66e4f659f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_model = MultiHeadSelfAttention(vector_size, heads)\n",
        "attention_model(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQcVN0zT67zV",
        "outputId": "362d66bf-cdc0-48ec-a036-9099ce735429"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_vTpFyBW7a-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}