{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7CXAroOnXyM",
        "outputId": "ee111448-715f-41fa-e546-556685c29660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (23.0)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.19.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "aI46gotXKY-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQB4BfBHKId9"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(tf.keras.Model):\n",
        "  def __init__(self, vector_size, heads=1):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.vector_size = vector_size\n",
        "    self.heads = heads\n",
        "    self.key = tf.keras.layers.Dense(vector_size // heads)\n",
        "    self.value = tf.keras.layers.Dense(vector_size // heads)\n",
        "    self.query = tf.keras.layers.Dense(vector_size // heads)\n",
        "\n",
        "  def call(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    sequence_length = x.shape[1]\n",
        "    x_mh = tf.reshape(x, [batch_size, sequence_length, self.heads, self.vector_size // self.heads])\n",
        "    x_mh = tf.transpose(x_mh, (0, 2, 1, 3))\n",
        "    key = tf.keras.layers.Dense(self.vector_size // self.heads)(x_mh)\n",
        "    value = tf.keras.layers.Dense(self.vector_size // self.heads)(x_mh)\n",
        "    query = tf.keras.layers.Dense(self.vector_size // self.heads)(x_mh)\n",
        "    mat_mul = tf.matmul(query, key, transpose_b=True)\n",
        "    n = self.vector_size\n",
        "    atten = mat_mul / tf.sqrt(tf.cast(n, tf.float32))\n",
        "    atten = tf.nn.softmax(atten)\n",
        "    # 乘上v\n",
        "    y = tf.matmul(atten, value)\n",
        "    y = tf.transpose(y, (0, 2, 1, 3))\n",
        "    y = tf.reshape(y, (batch_size, sequence_length, self.vector_size))\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "sequence_length = 10\n",
        "vector_size = 32\n",
        "heads = 4\n",
        "\n",
        "x = tf.random.uniform((batch_size, sequence_length, vector_size))\n",
        "attention_model = MultiHeadSelfAttention(vector_size, heads)\n",
        "attention_model(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZAvCQcCKU8g",
        "outputId": "b740bd3c-a880-4e88-c643-016b3958dad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上是multi-head self attention的输出，然后和x自身的residual做layer norm输入到FFN"
      ],
      "metadata": {
        "id": "dAFUvhJ0R0_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalization\n",
        "\n",
        "Layer Normalization vs Batch Normalization\n",
        "\n",
        "BN是针对每个batch做Normalization\n",
        "\n",
        "LN是针对层中的隐变量做Normalization。如x.shape=(4, 10, 32), LN是对32进行norm"
      ],
      "metadata": {
        "id": "clQfVIrARpWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11h6C0IuP63c",
        "outputId": "9ba887c9-4df2-4e13-df41-b77758986dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
              "array([0.8807533 , 0.23969948, 0.9159522 , 0.8483242 , 0.88680434,\n",
              "       0.5049244 , 0.29790604, 0.20629406, 0.31995618, 0.7408869 ,\n",
              "       0.9190035 , 0.8543589 , 0.6024481 , 0.13442862, 0.95582974,\n",
              "       0.987481  , 0.988533  , 0.3654201 , 0.7219895 , 0.34775913,\n",
              "       0.31657326, 0.82768834, 0.59613705, 0.8927474 , 0.7903615 ,\n",
              "       0.856418  , 0.4400022 , 0.76604843, 0.8117665 , 0.87231755,\n",
              "       0.35432923, 0.31857657], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(x[0][0]), np.std(x[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_UNmEahP8vz",
        "outputId": "9aa8a007-9d90-4aa4-aa32-4eebe21d10e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6425537, 0.26949573)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer Normalization\n",
        "ln = tf.keras.layers.LayerNormalization()\n",
        "y = ln(x)\n",
        "np.mean(y[0][0]), np.std(y[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-edlV7dRIWP",
        "outputId": "69cd822a-447e-449a-b303-3c32e7de84a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8.195639e-08, 0.99318594)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FFN的过程：\n",
        "\n",
        "1. 将vector_size扩大4倍：[batch_size, sequence_length, vector_size * 4]\n",
        "2. 通过非线性激活函数\n",
        "3. 还原vector_size: [batch_size, sequence_length, vector_size]\n"
      ],
      "metadata": {
        "id": "KJcqMppsN1Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vector_size, heads=1):\n",
        "    super().__init__()\n",
        "    # 两个LN层，因为每个都有独立的参数，所以无法复用\n",
        "    self.ln0 = tf.keras.layers.LayerNormalization()\n",
        "    self.ln1 = tf.keras.layers.LayerNormalization()\n",
        "    self.mh_atten = MultiHeadSelfAttention(vector_size, heads)\n",
        "    self.ffn = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(vector_size * 4),\n",
        "        tf.keras.layers.Activation(tfa.activations.gelu),\n",
        "        tf.keras.layers.Dense(vector_size),\n",
        "    ])\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    z = self.ln0(x + self.mh_atten(x))\n",
        "    y = self.ln1(z + self.ffn(z))\n",
        "    return y"
      ],
      "metadata": {
        "id": "qVu2_-94SkOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(vector_size, heads)\n",
        "transformer(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbp94JxsozE7",
        "outputId": "cb076625-f3a8-4325-a99a-74003f045d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}